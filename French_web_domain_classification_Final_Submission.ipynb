{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "French_web_domain_classification_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYCtwD_SR1dl",
        "colab_type": "text"
      },
      "source": [
        "#Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb5wAxMcRjRz",
        "colab_type": "code",
        "outputId": "2262be8d-4cbe-4fae-f682-8395a03d95c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=13q11ZnhYehDtibrHCvA7kfSkCKcxMpcv\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"fr-domain-classification.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"\")\n",
        "    "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13q11ZnhYehDtibrHCvA7kfSkCKcxMpcv\n",
            "To: /content/fr-domain-classification.zip\n",
            "259MB [00:02, 125MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZLkqKZ1RnPP",
        "colab_type": "code",
        "outputId": "5b9d20d1-0d5b-4339-c280-ba00a34b438c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\r\u001b[K     |▍                               | 10kB 23.7MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 4.8MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 6.6MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51kB 5.6MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 6.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71kB 7.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81kB 8.3MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 102kB 7.3MB/s eta 0:00:01\r\u001b[K     |███▊                            | 112kB 7.3MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 7.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 133kB 7.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 143kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 163kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 174kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 194kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 204kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 225kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 235kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 256kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 266kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 286kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 296kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 317kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 327kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 348kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 358kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 378kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 389kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 409kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 419kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 440kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 450kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 471kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 481kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 501kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 512kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 532kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 542kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 563kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 573kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 593kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 604kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 624kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 634kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 655kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 665kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 686kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 696kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 716kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 727kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 747kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 757kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 768kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 778kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 788kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 798kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 808kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 819kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 829kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 839kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 849kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 860kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 870kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 880kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 890kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 901kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 911kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 921kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 931kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 942kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 952kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 962kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 972kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 983kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.12.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993193 sha256=22a1fe9baaeede462950aa268e087be82331f3a53e40609e5c1e7a4fd87017f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3kwcPzARpAn",
        "colab_type": "code",
        "outputId": "f68d6bfe-4006-4b77-e179-bbc1e66289b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install translate"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting translate\n",
            "  Downloading https://files.pythonhosted.org/packages/85/b2/2ea329a07bbc0c7227eef84ca89ffd6895e7ec237d6c0b26574d56103e53/translate-3.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from translate) (2.21.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from translate) (7.0)\n",
            "Collecting pre-commit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/42/3f6698ff1bf3297eb5ba586635ce1a4b93f9012b1a181e09fd874790bcbb/pre_commit-2.1.1-py2.py3-none-any.whl (170kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 11.0MB/s \n",
            "\u001b[?25hCollecting tox\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/21/aa738f33db84be2caf89fae9d868320f7fc004329dd681cec4056d08cf75/tox-3.14.5-py2.py3-none-any.whl (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from translate) (4.2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->translate) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->translate) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->translate) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->translate) (2019.11.28)\n",
            "Collecting virtualenv>=15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/6a/6266c56d19c935c5fd7384e7d23936cbda6ecd0997ea3f3fbe9c9464f177/virtualenv-20.0.8-py2.py3-none-any.whl (4.6MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6MB 40.6MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/d9/ea9816aea31beeadccd03f1f8b625ecf8f645bd66744484d162d84803ce5/PyYAML-5.3.tar.gz (268kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 52.8MB/s \n",
            "\u001b[?25hCollecting nodeenv>=0.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/08/43/86ff33286c83f7b5e8903c32db01fe122c5e8a9d8dc1067dcaa9be54a033/nodeenv-1.3.5-py2.py3-none-any.whl\n",
            "Collecting importlib-resources; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/42/e8/5f65011c40a9281418179e297cb95e5a9a2c72c0d5ab99ee081acc8483ef/importlib_resources-1.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pre-commit->translate) (1.5.0)\n",
            "Collecting cfgv>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6d/82/49913e721128ff16d6b7cf304f513de7bba698583b045dfb9c4b3bb2f467/cfgv-3.1.0-py2.py3-none-any.whl\n",
            "Collecting identify>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/08/ebd9cd04a6741b2cdf9a634170070d16085a38c2041e11a9b634c1cef623/identify-1.4.11-py2.py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.7MB/s \n",
            "\u001b[?25hCollecting toml\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/12/ced7105d2de62fa7c8fb5fce92cc4ce66b57c95fb875e9318dba7f8c5db0/toml-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: py<2,>=1.4.17 in /usr/local/lib/python3.6/dist-packages (from tox->translate) (1.8.1)\n",
            "Collecting six<2,>=1.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: packaging>=14 in /usr/local/lib/python3.6/dist-packages (from tox->translate) (20.1)\n",
            "Collecting pluggy<1,>=0.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: filelock<4,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from tox->translate) (3.0.12)\n",
            "Collecting distlib<1,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/29/694a3a4d7c0e1aef76092e9167fbe372e0f7da055f5dcf4e1313ec21d96a/distlib-0.3.0.zip (571kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 45.7MB/s \n",
            "\u001b[?25hCollecting appdirs<2,>=1.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.7\"->pre-commit->translate) (3.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=14->tox->translate) (2.4.6)\n",
            "Building wheels for collected packages: pyyaml, distlib\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3-cp36-cp36m-linux_x86_64.whl size=44229 sha256=4c69ddead5566831284c24ee84c5f2346d67a3c155ae1cf17e154f1f46f17abb\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/76/4d/a95b8dd7b452b69e8ed4f68b69e1b55e12c9c9624dd962b191\n",
            "  Building wheel for distlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distlib: filename=distlib-0.3.0-cp36-none-any.whl size=340429 sha256=6d08b084377642e3b9a7cabdbc625704011da21d9f7384e1e974ba4aff0ac436\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/e8/db/c73dae4867666e89ba3cfbc4b5c092446f0e584eda6f409cbb\n",
            "Successfully built pyyaml distlib\n",
            "\u001b[31mERROR: pytest 3.6.4 has requirement pluggy<0.8,>=0.5, but you'll have pluggy 0.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: distlib, appdirs, importlib-resources, six, virtualenv, pyyaml, nodeenv, cfgv, identify, toml, pre-commit, pluggy, tox, translate\n",
            "  Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "Successfully installed appdirs-1.4.3 cfgv-3.1.0 distlib-0.3.0 identify-1.4.11 importlib-resources-1.3.1 nodeenv-1.3.5 pluggy-0.13.1 pre-commit-2.1.1 pyyaml-5.3 six-1.14.0 toml-0.10.0 tox-3.14.5 translate-3.5.0 virtualenv-20.0.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to6og6fKR-D5",
        "colab_type": "text"
      },
      "source": [
        "#Load dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1oA9pBNRu1_",
        "colab_type": "code",
        "outputId": "84e031d5-9c0d-4432-ed4b-8e376609b406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "import codecs\n",
        "from os import path\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import gensim \n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from langdetect import detect\n",
        "from translate import Translator\n",
        "from nltk.stem.snowball import FrenchStemmer\n",
        "import re\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jviULtBT5Zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#List Of English And French Stopwords\n",
        "stopwords =stopwords.words('french') + stopwords.words('english')\n",
        "#English And French Stemmer\n",
        "stemmer_french = FrenchStemmer()\n",
        "stemmer_english = SnowballStemmer(\"english\")\n",
        "\n",
        "#A RegexpTokenizer Splits A String Into Substrings Using A Regular Expression.\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "# Translator To Translate The Text Into French\n",
        "translator = Translator(to_lang=\"fr\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpRhvm5RT6rI",
        "colab_type": "text"
      },
      "source": [
        "#Load The Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rByX3AUNSBAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read training data\n",
        "with open(\"train.csv\", 'r') as f:\n",
        "    train_data = f.read().splitlines()\n",
        "\n",
        "train_hosts = list()\n",
        "y_train = list()\n",
        "for row in train_data:\n",
        "    host, label = row.split(\",\")\n",
        "    train_hosts.append(host)\n",
        "    y_train.append(label.lower())\n",
        "\n",
        "# Read test data\n",
        "with open(\"test.csv\", 'r') as f:\n",
        "    test_hosts = f.read().splitlines()\n",
        "\n",
        "# Load the textual content of a set of webpages for each host. \n",
        "# The encoding parameter is required since the majority of our text is french.\n",
        "train_data = list()\n",
        "for host in train_hosts:\n",
        "    try :\n",
        "        with open('text/text/' + host,encoding=\"utf-8\") as f: \n",
        "                text = f.read().replace(\"\\n\", \"\").lower()\n",
        "    except :\n",
        "        with open('text/text/' + host,encoding=\"latin-1\") as f: \n",
        "                text = f.read().replace(\"\\n\", \"\").lower()\n",
        "    train_data.append(text)\n",
        "\n",
        "# Get textual content of web hosts of the test set\n",
        "test_data = list()\n",
        "for host in test_hosts:\n",
        "    try :\n",
        "        with open('text/text/' + host,encoding=\"utf-8\") as f: \n",
        "                text = f.read().replace(\"\\n\", \"\").lower()\n",
        "    except :\n",
        "        with open('text/text/' + host,encoding=\"latin-1\") as f: \n",
        "                text = f.read().replace(\"\\n\", \"\").lower()\n",
        "    test_data.append(text)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0KiOI-gU_nP",
        "colab_type": "text"
      },
      "source": [
        "#Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu9MdLjRTYVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_preprocessing (string) : \n",
        "\n",
        "    output = string.replace(\"_\",\" \")\n",
        "    #Remove non-English characters, by selecting characters not in a given ASCII range \n",
        "    output=re.sub(r'[\\u4e00-\\u9fff]+',\" \", output)\n",
        "\n",
        "    output = ''.join(c for c in output if not c.isdigit())\n",
        "\n",
        "    #A tokenizer that splits a string using a regular expression, which matches either the tokens or the separators between tokens.\n",
        "\n",
        "    word_tokens = tokenizer.tokenize(output)\n",
        "    text = []\n",
        "    for w in word_tokens:\n",
        "        if w not in stopwords and len(w)>2:\n",
        "            text.append(w)\n",
        "            \n",
        "    #Stemmers remove morphological affixes from words, leaving only the word stem.\n",
        "    text = list(map(lambda x :stemmer_french.stem(x),text))\n",
        "    text = list(map(lambda x :stemmer_english.stem(x),text))\n",
        "\n",
        "    #The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank. \n",
        "    return TreebankWordDetokenizer().detokenize(text) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1rV-n6-TBh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Pytranslate is a state-of-art, well-equipped center, providing total solutions to customers in all translation, data conversion and pre-press operations like translation, multilingual typesetting and content management.\n",
        "#http://www.pytranslate.com/\n",
        "def translate(tokens) :\n",
        "    \n",
        "    string = TreebankWordDetokenizer().detokenize(tokens)\n",
        "    \n",
        "    if len(string) >2 :\n",
        "        #Port of Nakatani Shuyo's language-detection  to detect the language of the text.\n",
        "        if not detect(string) == 'fr' :\n",
        "            text = tokenizer.tokenize(translator.translate(string))\n",
        "        else :\n",
        "            text = tokenizer.tokenize(string)\n",
        "    else :\n",
        "        text = tokenizer.tokenize(string)\n",
        "        \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEWGYeZzT2bE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Translate And Tokenize Training Data\n",
        "train_data = list(map(text_preprocessing,train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_3yf5_nU6Zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Translate And Tokenize Test Data\n",
        "test_data = list(map(text_preprocessing,test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dPHiUeXV3SF",
        "colab_type": "text"
      },
      "source": [
        "# Term Frequency–Inverse Document Frequency,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q-zLEfXVuBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "vec = TfidfVectorizer(decode_error='ignore', strip_accents='unicode', min_df=1, max_df=500,sublinear_tf=True,stop_words=stopwords)\n",
        "X_train = vec.fit_transform(train_data)\n",
        "\n",
        "# Create the test matrix following the same approach as in the case of the training matrix\n",
        "X_test = vec.transform(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_TI2pruV_aD",
        "colab_type": "text"
      },
      "source": [
        "# Train The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDBDgcf3WB2a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "976ef2b1-c9f9-4c87-9155-cbd34fed3c85"
      },
      "source": [
        "#Linear classifiers (SVM, logistic regression, a.o.) with SGD training.\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "\n",
        "clf = SGDClassifier(loss='log', penalty='l2', tol=None, learning_rate= 'optimal', eta0=0.15, class_weight= None, alpha= 0.00001)\n",
        "#Fit linear model with Stochastic Gradient Descent.\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        " \t\n",
        "\n",
        "#Predict class labels for samples in X_train.\n",
        "print(clf.score(X_train, y_train))\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9411764705882353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_P0jVETWIO3",
        "colab_type": "text"
      },
      "source": [
        "# Load Pretrained Node Embeddings Matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiW4ZUHsWVXn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "d460b4ff-8fd6-4a09-fb33-8f6b7515cae4"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1E6K2bW5I8X0zIrXmQRy-c02-lZVKMl6n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1E6K2bW5I8X0zIrXmQRy-c02-lZVKMl6n\n",
            "To: /content/pretrained_embeddings.npy\n",
            "57.4MB [00:00, 183MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdNTG378We_O",
        "colab_type": "text"
      },
      "source": [
        "#Categorical Encoding Using Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e49sSmlNWfkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Encode target labels with value between 0 and n_classes-1.\n",
        "#This transformer should be used to encode target values, i.e. y, and not the input X.\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y_train)\n",
        "y_train_num = le.transform(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs03nPv1W-Dc",
        "colab_type": "text"
      },
      "source": [
        "#Train Node   And Test Node"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfLGouJeW5H8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings=np.load(\"pretrained_embeddings.npy\")\n",
        "idx=[int(u) for u in train_hosts]\n",
        "idx_test=[int(u) for u in test_hosts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N24R4GiVWvhE",
        "colab_type": "text"
      },
      "source": [
        "# Train The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvLrOI5LWc6I",
        "colab_type": "code",
        "outputId": "9fb05675-4699-4b22-ac7d-32b4418cadb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "#C-Support Vector Classification.\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "classifier = SVC(kernel='poly',degree=4,probability=True)\n",
        "#Fit the SVM model according to the given training data.\n",
        "classifier.fit(embeddings[idx], y_train_num)\n",
        "\n",
        "#Return the mean accuracy on the given test data and labels.\n",
        "print(classifier.score(embeddings[idx], y_train_num))\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8649411764705882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-mQQWpKY1Io",
        "colab_type": "text"
      },
      "source": [
        "# Weighted Average"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q61qetqY3hV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A hyperparameter for the Weighted Average\n",
        "alpha=0.6\n",
        "#The final Prediction\n",
        "y_pred=((1-alpha)*classifier.predict_proba(embeddings[idx_test])+(alpha)*clf.predict_proba(X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzGag3ACXQ4B",
        "colab_type": "text"
      },
      "source": [
        "# Kaggle submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZAk3R89XO1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write predictions to a file\n",
        "with open('text_baseline_'+str(alpha)+'.csv', 'w') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',')\n",
        "    lst = clf.classes_.tolist()\n",
        "    lst.insert(0, \"Host\")\n",
        "    writer.writerow(lst)\n",
        "    for i,test_host in enumerate(test_hosts):\n",
        "        lst = y_pred[i,:].tolist()\n",
        "        lst.insert(0, test_host)\n",
        "        writer.writerow(lst)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}